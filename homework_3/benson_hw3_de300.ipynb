{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795da8bf",
   "metadata": {},
   "source": [
    "## DE300 - Homework 3\n",
    "### Aden Benson\n",
    "#### Part 1 - tf-idf\n",
    "\n",
    "For the following code block, AI was used behind the scenes in the root shell of my docker container to install Java. I could either have done this by editing the dockerfile and rebuilding the image, or going into the shell and running some Java-related commands to install and link to the right location. I chose to do the second option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b471a619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/26 19:39:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col, explode, lower, length\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"AG news\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "agnews = spark.read.csv(\"dataset/agnews_clean.csv\", inferSchema=True, header=True)\n",
    "\n",
    "# turning the second column from a string to an array\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "agnews = agnews.withColumn('filtered', F.from_json('filtered', ArrayType(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87289959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+\n",
      "|_c0|                      filtered|\n",
      "+---+------------------------------+\n",
      "|  0|[wall, st, bears, claw, bac...|\n",
      "|  1|[carlyle, looks, toward, co...|\n",
      "|  2|[oil, economy, cloud, stock...|\n",
      "|  3|[iraq, halts, oil, exports,...|\n",
      "|  4|[oil, prices, soar, time, r...|\n",
      "+---+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 19:39:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/adenbenson/Desktop/spring_25/Data_Eng%20300/homeworks/hw3/dataset/agnews_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# each row contains the document id and a list of filtered words\n",
    "agnews.show(5, truncate=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b10f9",
   "metadata": {},
   "source": [
    "Task 1 - Design the MapReduce function for calculating tf-idf measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "769d93cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|_c0|     word|\n",
      "+---+---------+\n",
      "|  0|     wall|\n",
      "|  0|       st|\n",
      "|  0|    bears|\n",
      "|  0|     claw|\n",
      "|  0|     back|\n",
      "|  0|    black|\n",
      "|  0|  reuters|\n",
      "|  0|  reuters|\n",
      "|  0|    short|\n",
      "|  0|  sellers|\n",
      "|  0|     wall|\n",
      "|  0|   street|\n",
      "|  0|dwindling|\n",
      "|  0|     band|\n",
      "|  0|    ultra|\n",
      "|  0|   cynics|\n",
      "|  0|   seeing|\n",
      "|  0|    green|\n",
      "|  1|  carlyle|\n",
      "|  1|    looks|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 19:39:46 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/adenbenson/Desktop/spring_25/Data_Eng%20300/homeworks/hw3/dataset/agnews_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a pyspark df that contains each word and its associated document. This is what we'll perform mapreduce on.\n",
    "words=agnews.select(col('_c0'), explode(col('filtered')).alias('word'))\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d7604",
   "metadata": {},
   "source": [
    "First, design mapreduce functions for finding \"For each $d$, the counts of $t$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68d6a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design the map phase, which reads a row and returns the key of (word, doc number) and a value of 1\n",
    "def map_phase_1(row):\n",
    "    dict=row.asDict()\n",
    "    doc_num=dict['_c0']\n",
    "    word_str=dict['word']\n",
    "\n",
    "    # Return a tuple of ((doc_num, word), 1)\n",
    "    return ((doc_num, word_str), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b295d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design the sort/reduce phase, which adds up all the same instances of (doc_num, word) to find the counts.\n",
    "doc_word_counts=(words\n",
    "        .rdd\n",
    "        .map(map_phase_1)\n",
    "        .reduceByKey(lambda a,b: a+b)\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5857dc8",
   "metadata": {},
   "source": [
    "The above code outputs text files that have keys of document number and word, along with the count of that word in the document. This gives us \"For each $d$, the counts of $t$\"\n",
    "\n",
    "Next, design Mapreduce to create an rdd with the number of words in each doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "203706f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using words, create key value pairs of (doc_number, 1) for each word that appears.\n",
    "def map_phase_2(row):\n",
    "    dict=row.asDict()\n",
    "    doc_num=dict['_c0']\n",
    "    return (doc_num, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5fd5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design a reduce function that finds number of words per document\n",
    "num_words_per_doc=(words\n",
    "                   .rdd\n",
    "                   .map(map_phase_2)\n",
    "                   .reduceByKey(lambda a,b: a+b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "badf6952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 19:39:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/adenbenson/Desktop/spring_25/Data_Eng%20300/homeworks/hw3/dataset/agnews_clean.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 18.0), (9, 19.0), (18, 21.0), (27, 21.0), (36, 26.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a much smaller rdd, so we can show that it successfully counts the number of words in each document.\n",
    "num_words_per_doc.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3be0ae",
   "metadata": {},
   "source": [
    "Finally, use Mapreduce to find the counts of $d$ that contain $t$ for each $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "522ebbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a Words df that only contains unique values for each document\n",
    "# This gets rid of entries where documents have duplicate word entries\n",
    "unique_words=words.select(col('_c0'), col('word')).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acfe6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design map phase, which returns (word, 1) each time a word appears in a row\n",
    "def map_phase_3(row):\n",
    "    dict=row.asDict()\n",
    "    word=dict['word']\n",
    "    return (word, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96dc1008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 19:39:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/adenbenson/Desktop/spring_25/Data_Eng%20300/homeworks/hw3/dataset/agnews_clean.csv\n",
      "[Stage 6:======>                                                    (1 + 8) / 9]\r"
     ]
    }
   ],
   "source": [
    "# Design reduce phase, which finds the count that each word appears in a document\n",
    "word_counts=(\n",
    "    unique_words\n",
    "    .rdd\n",
    "    .map(map_phase_3)\n",
    "    .reduceByKey(lambda a,b: a+b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94160f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('nation', 1598.0),\n",
       " ('eggs', 38.0),\n",
       " ('software', 3798.0),\n",
       " ('different', 410.0),\n",
       " ('7', 2614.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec089ddd",
   "metadata": {},
   "source": [
    "Task 2 - Calculate tf-idf measure for each row in the agnews_clean.csv. Save the measures in a new column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c5715",
   "metadata": {},
   "source": [
    "Let $t$ be a term (a word), $d$ be a document, and $D$ be the collection of the documents.\n",
    "\n",
    "Term frequency (tf):\n",
    "\n",
    "$$\\mathrm{tf}(t, d) = \\frac{\\textrm{\\# occurrences of } t \\textrm{ in } d}{\\textrm{\\# terms in } d},$$\n",
    "\n",
    "Inverse document frequency (idf): $$\\mathrm{idf}(t, D) = \\log\\left(\\frac{\\textrm{\\# docs in } D}{\\textrm{\\# docs containing } t}\\right).$$\n",
    "\n",
    "As a result, the tf-idf measure is\n",
    "\n",
    "$$\\textrm{tf-idf}(t, d, D) = \\mathrm{tf}(t, d)\\times \\mathrm{idf}(t, D).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d46ac491",
   "metadata": {},
   "outputs": [],
   "source": [
    "### First, calculate tf(t,d)\n",
    "\n",
    "# Add # terms in d to doc_word_counts, which already contains # occurrences of t in d\n",
    "\n",
    "# Start by changing format of doc_word_counts from ((doc, word), count) to (doc, (word, count)) so we can join along document key\n",
    "def transform_helper(row):\n",
    "    return (row[0][0], (row[0][1], row[1]))\n",
    "doc_word_counts_tf=doc_word_counts.map(transform_helper)\n",
    "\n",
    "# Join doc_word_counts along doc number with num_words_per_doc\n",
    "joined_tf=doc_word_counts_tf.join(num_words_per_doc)\n",
    "\n",
    "# Compute tf(t,d) term\n",
    "def tf_t_d(row):\n",
    "    tf_t_d_calc=row[1][0][1]/row[1][1]\n",
    "    return ((row[0], row[1][0][0]), tf_t_d_calc)\n",
    "computed_tf=joined_tf.map(tf_t_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba25c448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 19:40:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/adenbenson/Desktop/spring_25/Data_Eng%20300/homeworks/hw3/dataset/agnews_clean.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((0, 'st'), 0.05555555555555555)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we have computed_tf which shows #d#, #t#, and the tf(t,d) measure\n",
    "computed_tf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2da53a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 19:40:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: \n",
      " Schema: _c0\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/adenbenson/Desktop/spring_25/Data_Eng%20300/homeworks/hw3/dataset/agnews_clean.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Next, calculate idf(t,d)\n",
    "\n",
    "# Find number of docs in D\n",
    "num_docs=agnews.select(col('_c0')).distinct().count()\n",
    "\n",
    "# Add the idf(t,D) metric to each row of word_counts\n",
    "import numpy as np\n",
    "def idf_t_D(row):\n",
    "    idf_t_D_calc=np.log(num_docs/row[1])\n",
    "    return (row[0], idf_t_D_calc)\n",
    "computed_idf=word_counts.map(idf_t_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efa3afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finally, compute tf-idf measure\n",
    "\n",
    "# Transform computed_tf so the key is only on the word\n",
    "def tf_transform_helper(row):\n",
    "    return (row[0][1], (row[0][0], row[1]))\n",
    "transformed_computed_tf=computed_tf.map(tf_transform_helper)\n",
    "\n",
    "# Add idf measure to tf RDD\n",
    "full_rdd=transformed_computed_tf.join(computed_idf)\n",
    "\n",
    "# Compute tf-idf measure\n",
    "def compute_tf_idf(row):\n",
    "    tf_idf_calc=row[1][0][1]*row[1][1]\n",
    "    return ((row[1][0][0], row[0]), float(tf_idf_calc))\n",
    "tf_idf_rdd=full_rdd.map(compute_tf_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32f8686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adenbenson/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------------+\n",
      "|doc_Number|   word|     tf_idf_measure|\n",
      "+----------+-------+-------------------+\n",
      "|        18|deficit|  0.540640233790213|\n",
      "|     11970|deficit|0.24681401977379286|\n",
      "|     20826|deficit|0.23653010228321816|\n",
      "|     21078|deficit|0.23170295733866267|\n",
      "|     37116|deficit|0.24681401977379286|\n",
      "+----------+-------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform our RDD back into a pyspark DF to display our outputs\n",
    "\n",
    "# First, use map to remove nested tuples. Then, use .toDF to turn into a Pyspark Dataframe\n",
    "tf_idf_df=tf_idf_rdd.map(lambda i: (i[0][0], i[0][1], i[1])).toDF(['doc_Number', 'word', 'tf_idf_measure'])\n",
    "\n",
    "tf_idf_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af642d",
   "metadata": {},
   "source": [
    "Task 3 - Print out the tf-idf measure for the first 5 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f9ec558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------------+\n",
      "|doc_Number|     word|     tf_idf_measure|\n",
      "+----------+---------+-------------------+\n",
      "|         0|   seeing|0.37743394553516213|\n",
      "|         0|    short| 0.2773120373951269|\n",
      "|         0|     claw|  0.499114829314058|\n",
      "|         0|     back| 0.1892216338539946|\n",
      "|         0|    green| 0.2877107940095433|\n",
      "|         0|dwindling| 0.4572386180709258|\n",
      "|         0|    ultra| 0.4125512394225831|\n",
      "|         0|       st| 0.2584728642725166|\n",
      "|         0|  sellers| 0.4468379768438066|\n",
      "|         0|   street|0.24678348986493034|\n",
      "|         0|   cynics|  0.563734318747707|\n",
      "|         0|    bears| 0.3372044607529448|\n",
      "|         0|     wall| 0.5115985326511431|\n",
      "|         0|  reuters|0.24754017186645658|\n",
      "|         0|    black| 0.2953171727366614|\n",
      "|         0|     band| 0.3643421454792778|\n",
      "+----------+---------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+\n",
      "|doc_Number|      word|     tf_idf_measure|\n",
      "+----------+----------+-------------------+\n",
      "|         1|    placed| 0.2284965552404658|\n",
      "|         1|  industry|0.15043731768548949|\n",
      "|         1| aerospace| 0.2581171817448437|\n",
      "|         1|   quietly|0.25188254045524316|\n",
      "|         1|     looks| 0.1973537176743789|\n",
      "|         1|      bets|0.27861293130724324|\n",
      "|         1|    toward| 0.1898997183872362|\n",
      "|         1|   carlyle| 0.7168306746824437|\n",
      "|         1|      firm|0.15969712503706046|\n",
      "|         1|   defense| 0.1751279339938823|\n",
      "|         1|     plays|0.22418048797172685|\n",
      "|         1|investment| 0.1890771769001148|\n",
      "|         1|commercial| 0.2057832028092643|\n",
      "|         1|    market|0.13394932212703356|\n",
      "|         1|   private| 0.1929050573011279|\n",
      "|         1|reputation| 0.2578098186776328|\n",
      "|         1|     timed|  0.324478643568105|\n",
      "|         1|    making| 0.1698717076460444|\n",
      "|         1|     group|0.12468100563149095|\n",
      "|         1|      part|0.16022031730914288|\n",
      "+----------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------------+\n",
      "|doc_Number|    word|     tf_idf_measure|\n",
      "+----------+--------+-------------------+\n",
      "|         2|expected|0.16094627131903613|\n",
      "|         2|earnings| 0.1792714404894228|\n",
      "|         2|   stock|0.17879168082328206|\n",
      "|         2|  prices|0.14472559202114177|\n",
      "|         2|    hang|0.30475018305843793|\n",
      "|         2| worries|0.23009353850726894|\n",
      "|         2|  stocks|0.14976769101715193|\n",
      "|         2|   depth|0.31343954772064864|\n",
      "|         2| outlook| 0.4265073217271922|\n",
      "|         2|     oil|0.13908157105107033|\n",
      "|         2|  summer|0.22694739048609625|\n",
      "|         2|  market|0.15069298739291276|\n",
      "|         2|    next|0.14062721303262238|\n",
      "|         2| soaring| 0.2596334462817101|\n",
      "|         2|   crude|  0.197241148492091|\n",
      "|         2| economy| 0.3721400726458204|\n",
      "|         2|    week|0.13121900794126834|\n",
      "|         2|    plus|0.24449073714833106|\n",
      "|         2|   cloud|  0.295159450642955|\n",
      "|         2| reuters|0.18565512889984243|\n",
      "+----------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------------+\n",
      "|doc_Number|          word|     tf_idf_measure|\n",
      "+----------+--------------+-------------------+\n",
      "|         3|          iraq|0.23809526243476142|\n",
      "|         3|        showed| 0.1743365558077232|\n",
      "|         3|  intelligence|0.20782569445751425|\n",
      "|         3|        strike|0.17411586950893898|\n",
      "|         3|      official|0.15149485319300557|\n",
      "|         3|         flows| 0.2774168429760197|\n",
      "|         3|          main|0.36492623402353547|\n",
      "|         3|          said|0.06593367258642661|\n",
      "|         3|         rebel|0.18209445014364567|\n",
      "|         3|       militia| 0.2252006141545402|\n",
      "|         3|           oil|0.35763832555989516|\n",
      "|         3|      pipeline| 0.4720829409342409|\n",
      "|         3|infrastructure|0.22959926718225876|\n",
      "|         3|        export|0.23862435123782139|\n",
      "|         3|   authorities|0.18159366801541998|\n",
      "|         3|      southern|  0.336553609483104|\n",
      "|         3|      saturday|0.12197305137253434|\n",
      "|         3|         halts|0.27365396741681164|\n",
      "|         3|        halted| 0.2557691357056513|\n",
      "|         3|       reuters|0.15913296762843637|\n",
      "+----------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:======================================>                 (20 + 9) / 29]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------------+\n",
      "|doc_Number|     word|     tf_idf_measure|\n",
      "+----------+---------+-------------------+\n",
      "|         4|     time|0.10623532598945136|\n",
      "|         4|       us| 0.1669859687392097|\n",
      "|         4|  present|0.22209684830286883|\n",
      "|         4|   record| 0.1232987151692413|\n",
      "|         4|   months|0.14002501854271598|\n",
      "|         4|     soar| 0.2306791247647116|\n",
      "|         4|   prices|0.23156094723382684|\n",
      "|         4|   barely|0.21935019724396657|\n",
      "|         4|  wallets| 0.2665151844733088|\n",
      "|         4|   menace| 0.5747440955975784|\n",
      "|         4| toppling|0.27964532733021175|\n",
      "|         4|      oil|0.22253051368171256|\n",
      "|         4| tearaway| 0.3918885216630942|\n",
      "|         4|      new| 0.1271397626254836|\n",
      "|         4|straining| 0.2904044404056468|\n",
      "|         4|  economy|0.14885602905832815|\n",
      "|         4|   posing| 0.2589223867776184|\n",
      "|         4| economic|0.14782686453681568|\n",
      "|         4|    world|0.09332201126546583|\n",
      "|         4|elections|0.16009904796740967|\n",
      "+----------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Sort our df by doc_Number, then print out all words in the first 5 documents with the tf-idf measure\n",
    "tf_idf_df_sorted=tf_idf_df.orderBy(col('doc_Number'))\n",
    "\n",
    "# Use a for loop to print out each df by doc_Number, since one table for all words was too large\n",
    "for i in range(5):\n",
    "    tf_idf_df_filtered=tf_idf_df_sorted.where(col('doc_Number')==i)\n",
    "    tf_idf_df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f82695",
   "metadata": {},
   "source": [
    "## 2. SVM objective function\n",
    "The soft-margin support vector machine classification model minimizes the following objective function (loss).\n",
    "\n",
    "With $\\{\\mathbf{x}_i, y_i\\}_{i=1}^n$ as the data, $\\mathbf{x}_i \\in \\mathbb{R}^d, y_i \\in \\{-1, 1\\},\\mathbf{w}$ as the weight vector, and $b$ as the bias:\n",
    "\n",
    "$$L(\\mathbf{w}, b) = \\lambda \\|\\mathbf{w}\\|^2 + n^{-1}\\sum_{i=1}^{n} \\max\\{0, 1 - y_i (\\mathbf{w}^\\mathsf{T} \\mathbf{x}_i + b)\\},$$\n",
    "\n",
    "where $\\|\\mathbf{w}\\|$ is the $L_2$ norm.\n",
    "\n",
    "The SVM classifier is: $$\\widehat{y}_i = \\mathrm{sgn}(\\mathbf{w}^\\mathsf{T} \\mathbf{x}_i + b),$$ where $\\mathrm{sgn}(z)$ returns the sign of $z$, i.e. $+1$ if positive, $-1$ if negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "584a6998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# First, put csvs into Pyspark\n",
    "svm_ps = spark.read.csv(\"dataset/data_for_svm.csv\", inferSchema=True, header=False)\n",
    "bias_ps = spark.read.csv(\"dataset/bias.csv\", inferSchema=True, header=False)\n",
    "w_ps = spark.read.csv(\"dataset/w.csv\", inferSchema=True, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910a6df",
   "metadata": {},
   "source": [
    "1. Design the MapReduce functions required to calculate the loss function.\n",
    "\n",
    "The work comes from trying to calculate this quantity:$$\\sum_{i=1}^{n} \\max\\{0, 1 - y_i (\\mathbf{w}^\\mathsf{T} \\mathbf{x}_i + b)\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2aa5118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 19:40:31 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# Design the map portion, which calculates the max(0, 1-y_i(w^T x_i +b)) for each row\n",
    "\n",
    "# First, define global variables outside the function\n",
    "# Get bias value\n",
    "bias_val=float(bias_ps.take(1)[0][0])\n",
    "\n",
    "# Get the w value as a list so we can do w^T * x_i\n",
    "w_vals=list(w_ps.take(1)[0])\n",
    "\n",
    "def map_part2(row):\n",
    "    dict=row.asDict()\n",
    "\n",
    "    x_col_names=[f'_c{i}' for i in range(64)]\n",
    "    y_col_name='_c64'\n",
    "\n",
    "    # Get x values\n",
    "    x_vals=[]\n",
    "    for label in x_col_names:\n",
    "        x_vals.append(dict[label])\n",
    " \n",
    "    # Get the y value\n",
    "    y_val=dict[y_col_name]\n",
    "\n",
    "    # Calculate 1-y_i(w^T * x_i + b)\n",
    "    calculation=1-y_val*(np.dot(np.array(w_vals), np.array(x_vals))+bias_val)\n",
    "\n",
    "    return ('loss_val', max(0, calculation))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c0e8a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "399889.5049647091"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Design reduce portion, which finds the sum of all max(0, 1-y_i(w^T x_i +b)) values\n",
    "from operator import add\n",
    "summed_total=svm_ps.rdd.map(map_part2).reduceByKey(add).collect()[0][1]\n",
    "summed_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bef726",
   "metadata": {},
   "source": [
    "2. Using these functions, create a function loss_SVM(w, b, X, y) to calculate the SVM objective for a given choice of w, b with data stored in X, y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1e88d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_SVM(w,b,X,y,lam):\n",
    "    ###\n",
    "    # Assuming w, b, X, and y are inputed as PySpark dataframes, and lam is inputed as a float value\n",
    "    ###\n",
    "    \n",
    "    # Get a list of w values\n",
    "    w_vals=list(w.take(1)[0])\n",
    "\n",
    "    # Get bias value\n",
    "    bias_val=float(b.take(1)[0][0])\n",
    "\n",
    "    # Get n^(-1) value\n",
    "    n_inv=1/X.count()\n",
    "\n",
    "    # Join X and y together so we can use our map function from above\n",
    "    from pyspark.sql.functions import monotonically_increasing_id\n",
    "    X=X.withColumn('id', monotonically_increasing_id())\n",
    "    y=y.withColumn('id', monotonically_increasing_id())\n",
    "\n",
    "    joined_data=X.join(y, on='id').drop('id')\n",
    "\n",
    "    # Same function as above\n",
    "    def map_part2(row):\n",
    "        dict=row.asDict()\n",
    "\n",
    "        x_col_names=[f'_c{i}' for i in range(64)]\n",
    "        y_col_name='_c64'\n",
    "\n",
    "        # Get x values\n",
    "        x_vals=[]\n",
    "        for label in x_col_names:\n",
    "            x_vals.append(dict[label])\n",
    "    \n",
    "        # Get the y value\n",
    "        y_val=dict[y_col_name]\n",
    "\n",
    "        # Calculate 1-y_i(w^T * x_i + b)\n",
    "        calculation=1-y_val*(np.dot(np.array(w_vals), np.array(x_vals))+bias_val)\n",
    "\n",
    "        return ('loss_val', max(0, calculation))\n",
    "        \n",
    "    # Again, same reduce function\n",
    "    summed_total=joined_data.rdd.map(map_part2).reduceByKey(add).collect()[0][1]\n",
    "\n",
    "    # Return loss objective function using values calculated above\n",
    "    return lam * np.linalg.norm(w_vals)**2 + n_inv * summed_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ecfed6",
   "metadata": {},
   "source": [
    "3. You are given the following dataset data_for_svm.csv, where the first 64 columns contain X and the last column contains y. Using the weights and bias provided in w.csv and bias.csv, calculate the objective value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9fb3b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 85:=====>                                                   (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value:  1.0029403834857487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create X and y dfs\n",
    "X = svm_ps.select(svm_ps.columns[:-1])\n",
    "y = svm_ps.select(svm_ps.columns[-1])\n",
    "\n",
    "print('Objective value: ', loss_SVM(w_ps, bias_ps, X, y,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8b9e4e",
   "metadata": {},
   "source": [
    "4. Design the MapReduce function required to make prediction. Predict for all of the data using the provided weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dea533a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define a mapping function to calculate y_hat for each data point\n",
    "\n",
    "# Get bias value\n",
    "bias_val=float(bias_ps.take(1)[0][0])\n",
    "\n",
    "# Get the w value as a list so we can do w^T * x_i\n",
    "w_vals=list(w_ps.take(1)[0])\n",
    "\n",
    "def map_predict(row):\n",
    "    dict=row.asDict()\n",
    "    \n",
    "    x_col_names=[f'_c{i}' for i in range(64)]\n",
    "\n",
    "    # Get X values\n",
    "    x_vals=[]\n",
    "    for label in x_col_names:\n",
    "        x_vals.append(dict[label])\n",
    "\n",
    "    \n",
    "    # Compute inner value in the y_hat score function\n",
    "    score=np.dot(np.array(w_vals), np.array(x_vals))+bias_val\n",
    "\n",
    "    # Make the prediction binary based on sign.\n",
    "    y_hat = 1 if score >= 0 else -1\n",
    "\n",
    "    return ('Prediction', y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2734b73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', -1),\n",
       " ('Prediction', 1),\n",
       " ('Prediction', 1),\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create reduce operation to apply the prediction\n",
    "preds=X.rdd.map(map_predict)\n",
    "preds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcb09df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 101:=====>                                                  (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of Predictions:  0.9579132416284583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# For comparison, find the loss of our predictions\n",
    "preds_df=preds.toDF().select(col('_2').alias('_c64'))\n",
    "print('Loss of Predictions: ', loss_SVM(w_ps, bias_ps, X, preds_df,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
